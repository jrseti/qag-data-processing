{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, fftfreq\n",
    "import scipy.fftpack\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data we will use.\n",
    "\n",
    "# Lower case dish names!\n",
    "data_file_path = '/Volumes/QAG Disk 1/data/moon_09102021'\n",
    "#data_files = ['5b_x.dat', '5b_y.dat', '1h_x.dat', '1h_y.dat']\n",
    "dish_names = ['1h', '5b']\n",
    "antenna_pols = ['x', 'y']\n",
    "\n",
    "DATA_FILE_EXTENSION = 'dat'\n",
    "\n",
    "# Dishes 1h and 5b. Their XYZ ECEF coordinates in meters are:\n",
    "# Lower case dish names!\n",
    "dishes = [{ 'name' : '1h',\n",
    "            'x' : '1h_x.dat',\n",
    "            'y' : '1h_y.dat',\n",
    "            'XYZ' : [-2524203.0583488033, -4123430.8743299064, 4147685.0269737383] },\n",
    "          { 'name' : '5b', \n",
    "            'x' : '5b_x.dat',\n",
    "            'y' : '5b_y.dat',\n",
    "            'XYZ' : [-2523924.2759636184, -4123447.7050980935, 4147836.6033990188] } ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the schedule\n",
    "schedule = []\n",
    "\n",
    "schedule.append([-3,3,'2021-09-10 23:05:00'])\n",
    "schedule.append([-6,6,'2021-09-10 23:07:00'])\n",
    "schedule.append([-9,9,'2021-09-10 23:09:00'])\n",
    "schedule.append([-10,10,'2021-09-10 23:11:00'])\n",
    "schedule.append([-11,11,'2021-09-10 23:13:00'])\n",
    "schedule.append([-13,13,'2021-09-10 23:15:00'])\n",
    "schedule.append([-3,6,'2021-09-10 23:17:00'])\n",
    "schedule.append([-3,9,'2021-09-10 23:19:00'])\n",
    "schedule.append([-3,10,'2021-09-10 23:21:00'])\n",
    "schedule.append([-3,11,'2021-09-10 23:23:00'])\n",
    "schedule.append([-3,13,'2021-09-10 23:25:00'])\n",
    "schedule.append([-6,3,'2021-09-10 23:27:00'])\n",
    "schedule.append([-6,9,'2021-09-10 23:29:00'])\n",
    "schedule.append([-6,10,'2021-09-10 23:31:00'])\n",
    "schedule.append([-6,11,'2021-09-10 23:33:00'])\n",
    "schedule.append([-6,13,'2021-09-10 23:35:00'])\n",
    "schedule.append([-9,3,'2021-09-10 23:37:00'])\n",
    "schedule.append([-9,6,'2021-09-10 23:39:00'])\n",
    "schedule.append([-9,10,'2021-09-10 23:41:00'])\n",
    "schedule.append([-9,11,'2021-09-10 23:43:00'])\n",
    "schedule.append([-9,13,'2021-09-10 23:45:00'])\n",
    "schedule.append([-10,3,'2021-09-10 23:47:00'])\n",
    "schedule.append([-10,6,'2021-09-10 23:49:00'])\n",
    "schedule.append([-10,9,'2021-09-10 23:51:00'])\n",
    "schedule.append([-10,11,'2021-09-10 23:53:00'])\n",
    "schedule.append([-10,13,'2021-09-10 23:55:00'])\n",
    "schedule.append([-11,3,'2021-09-10 23:57:00'])\n",
    "schedule.append([-11,6,'2021-09-10 23:59:00'])\n",
    "schedule.append([-11,9,'2021-09-11 00:01:00'])\n",
    "schedule.append([-11,10,'2021-09-11 00:03:00'])\n",
    "schedule.append([-11,13,'2021-09-11 00:05:00'])\n",
    "schedule.append([-13,3,'2021-09-11 00:07:00'])\n",
    "schedule.append([-13,6,'2021-09-11 00:09:00'])\n",
    "schedule.append([-13,9,'2021-09-11 00:11:00'])\n",
    "schedule.append([-13,10,'2021-09-11 00:13:00'])\n",
    "schedule.append([-13,11,'2021-09-11 00:15:00'])\n",
    "schedule.append([-10,10,'2021-09-11 00:17:00'])\n",
    "schedule.append([-9,9,'2021-09-11 00:19:00'])\n",
    "schedule.append([-8,8,'2021-09-11 00:21:00'])\n",
    "\n",
    "#print(schedule)\n",
    "print(len(schedule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the full path filename for a dish/pol\n",
    "def get_datafile_fullpath(dish_name, pol):\n",
    "    \"\"\"\n",
    "    Create the full path filename for a dish/pol\n",
    "    \n",
    "    Argumants:\n",
    "        dish_name - name of the dish, like 1h or 5b\n",
    "        pol       - 'x' or 'y' to specify the polarixation in the filename\n",
    "        \n",
    "    Returns:\n",
    "        The full path filename. None if dish_name is not in the list\n",
    "    \"\"\"\n",
    "    \n",
    "    if dish_name == None or len(dish_name) != 2:\n",
    "        return None\n",
    "    if pol != 'x' and pol != 'y':\n",
    "        return None\n",
    "    \n",
    "    for dish in dishes:\n",
    "        name = dish.get('name')\n",
    "        if name == dish_name.lower():\n",
    "            full_path = f\"{data_file_path}/{dish[pol]}\"\n",
    "            return full_path\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = 1631312409.1568253092447917 #This is UTC 2021-09-10 22:20:09\n",
    "SAMPLES_PER_SEC = 1920000\n",
    "BYTES_PER_SAMPLE = 4\n",
    "SECS_OFFSET = 3\n",
    "\n",
    "def seek_by_time(file, datetime_str):\n",
    "    \"\"\"Find the offset into the file for the given datetime and seek.\n",
    "    Arguments:\n",
    "        file - the open file\n",
    "        datetime_str - A datetime in the form 2021-09-10 22:20:09, this if UTC\n",
    "    Returns:\n",
    "        int - the bytes skipped in the file.\n",
    "    \"\"\"\n",
    "    d_start = dt.strptime(\"2021-09-10 22:20:09\", '%Y-%m-%d %H:%M:%S') \n",
    "    d = dt.strptime(datetime_str, '%Y-%m-%d %H:%M:%S') \n",
    "    diff_secs = int(dt.timestamp(d) - dt.timestamp(d_start)) + SECS_OFFSET\n",
    "\n",
    "    bytes_to_skip = diff_secs * SAMPLES_PER_SEC * BYTES_PER_SAMPLE\n",
    "    \n",
    "    #print(bytes_to_skip)\n",
    "    \n",
    "    if file != None:\n",
    "        file.seek(bytes_to_skip)\n",
    "    \n",
    "    return bytes_to_skip\n",
    "\n",
    "#if seek_by_time(None, \"2021-09-10 22:20:10\")  != 7680000:\n",
    "#    print(\"WARNING: seek_by_time() did not return the correct offset. Please check!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readBlock(file, num_samples):\n",
    "    \"\"\" \n",
    "    Read the next FFT sized block of 16-bit I/Q data from an opened file.\n",
    "    \n",
    "    Arguments:\n",
    "        file - The reference to the data file, already opened.\n",
    "        num_samples - The number of 16-bit I/Q samples to read in.\n",
    "            Example, If you FFT SIZE is 1024, the num_samples should\n",
    "            be 1024. 2048 16-bit samples (4096 bytes total) will be \n",
    "            read and unpacked from the file.\n",
    "            \n",
    "        Return - A Numpy array of \"num_samples\" Complex64 values.\n",
    "            None if the read is past the end of the file.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Read the bytes from the file\n",
    "    data_raw = np.fromfile(file, dtype=np.int16, count=num_samples*2)\n",
    "    \n",
    "    # Create a Complex64 Numpy array to hold the values\n",
    "    data = np.zeros((num_samples,), dtype=np.complex64)\n",
    "    \n",
    "    # Reassemble the int16 aray into the Complex64 array.\n",
    "    # Use a try block to catch the condition where the end of the file\n",
    "    # has been passed.\n",
    "    try:\n",
    "        data[:] = (data_raw[0::2]) + 1j*(data_raw[1::2])\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_waterfall(title1, title2, datetime_str, num_secs, start_chan, end_chan, output_image_name=None):\n",
    "    \n",
    "    \"\"\"Create a waterfall plot\n",
    "    Arguments:\n",
    "        filename - The full path filename\n",
    "        datetime_str - the time offset into the file\n",
    "        num_secs - number of seconds to process\n",
    "        start_chan - the start frequency bin\n",
    "        end_chan - the end frequency bin\n",
    "        output_image_name- Optional, save an image of the plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform the FFT and store the result for later plotting.\n",
    "    # The FFT result will be accumulated in a one dimensional Numpy array\n",
    "    # of length equal to the FFT_SIZE\n",
    "\n",
    "    # Define some constants. Here you may wish to change the FFT_SIZE\n",
    "    FFT_SIZE = 1960000\n",
    "    CENTER_FREQ_HZ = 1296000000\n",
    "    BANDWIDTH_HZ   = 1920000\n",
    "    MHZ = 1048576.0\n",
    "    NUM_CHANS = end_chan - start_chan\n",
    "    # How many FFT_SIZE chunks to process. Ideally we would like to\n",
    "    # process the entire channel, but during code development we may\n",
    "    # wish to read in a smaller number. Reading in the large data files\n",
    "    # takes a lot of time.\n",
    "    NUM = num_secs\n",
    "    \n",
    "    fig, axs = plt.subplots(len(dish_names),len(antenna_pols), figsize=(72/8, 60/8))\n",
    "    \n",
    "    axis_counter = 0\n",
    "    \n",
    "    for dish_name in dish_names:\n",
    "        for pol in antenna_pols:\n",
    "            \n",
    "            file_path = get_datafile_fullpath(dish_name, pol)\n",
    "            #print(file_path)\n",
    "            \n",
    "            # Create an array to hold the data for each horizontal line of the waterfall\n",
    "            bins = []\n",
    "\n",
    "            # Open the file\n",
    "            file = open(file_path, \"rb\")\n",
    "\n",
    "            # Seek\n",
    "            seek_by_time(file, datetime_str)\n",
    "    \n",
    "            # Create an array to hold the accumulation for each bin\n",
    "            #fft_acc = np.zeros((end_chan - start_chan, ))\n",
    "\n",
    "            # Keep a counter of the number of FFT_SIZE length chunks we processed.\n",
    "            count = 0\n",
    "\n",
    "            # Loop through the file. \n",
    "            for i in range(NUM):\n",
    "\n",
    "                #print(i)\n",
    "\n",
    "                # Read the next block of data from the file\n",
    "                d = readBlock(file, FFT_SIZE)\n",
    "\n",
    "                # If None is retruned, this signifies we read to the end of the file.\n",
    "                # Break out of the loop.\n",
    "                if d is None:\n",
    "                    print(\"d is NONE\")\n",
    "                    break\n",
    "\n",
    "                yf = scipy.fftpack.fft(d, n=FFT_SIZE, axis=0)\n",
    "\n",
    "                #Add the results of the FFTp to the fft_acc Numpy array\n",
    "                #np.add(np.abs(yf[start_chan:end_chan]), fft_acc, out=fft_acc)\n",
    "\n",
    "                #print(np.abs(yf[start_chan:start_chan+1]), fft_acc[0:1])\n",
    "\n",
    "                xx = np.abs(yf).tolist()\n",
    "                yy = scipy.fft.fftshift(xx)\n",
    "                bins.append(yy[start_chan:end_chan])\n",
    "\n",
    "            # Draw a plot of the results\n",
    "            \n",
    "            col = axis_counter % len(axs)\n",
    "            row = int(axis_counter / len(axs))\n",
    "            #print(row, col)\n",
    "\n",
    "            axs[row, col].imshow(bins, aspect='auto')\n",
    "    \n",
    "            axs[row,col].set_title(title2)\n",
    "    \n",
    "            axs[row, col].set_title(f'{dish_name} - {pol} [{title1}]', fontsize= 16, fontweight=\"bold\")\n",
    "            \n",
    "            if row == 1:\n",
    "                axs[row, col].set_xlabel('Hz', fontsize=16)\n",
    "            \n",
    "            if col == 0:\n",
    "                axs[row, col].set_ylabel('Seconds', fontsize=16)\n",
    "            \n",
    "            axis_counter += 1\n",
    "\n",
    "    fig.tight_layout()\n",
    "    #fig.suptitle(title1, fontsize=18)\n",
    "    #plt.subplots_adjust(top=2.0)\n",
    "    #plt.title(title1, fontsize=18)\n",
    "\n",
    "    if output_image_name != None:\n",
    "        plt.savefig(output_image_name)\n",
    "        \n",
    "    #plt.text(-3.5, -3.5, title1, fontsize=24)\n",
    "        \n",
    "    # Show!\n",
    "    #plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_waterfall(\"+3/-3 Hz\", \"  \", \"2021-09-10 23:05:00\", 60, 1236524, 1236622, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '1h', 'x': '1h_x.dat', 'y': '1h_y.dat', 'XYZ': [-2524203.0583488033, -4123430.8743299064, 4147685.0269737383]}, {'name': '5b', 'x': '5b_x.dat', 'y': '5b_y.dat', 'XYZ': [-2523924.2759636184, -4123447.7050980935, 4147836.6033990188]}]\n"
     ]
    }
   ],
   "source": [
    "print(dishes)\n",
    "for dish_info in dishes:\n",
    "    #print(dish_info)\n",
    "    dish_name = dish_info['name']\n",
    "    file_path = get_datafile_fullpath(dish_name, 'x')\n",
    "    file_path = get_datafile_fullpath(dish_name, 'y')\n",
    "    #create_waterfall(file_path, \"3, -3\", \"  \", \"2021-09-10 23:05:00\", 60, 1236524, 1236622, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10_+10 Hz images/plots_-10_+10_hz.png\n",
      "-11_+11 Hz images/plots_-11_+11_hz.png\n",
      "-13_+13 Hz images/plots_-13_+13_hz.png\n",
      "-3_+6 Hz images/plots_-3_+6_hz.png\n",
      "-3_+9 Hz images/plots_-3_+9_hz.png\n",
      "-3_+10 Hz images/plots_-3_+10_hz.png\n",
      "-3_+11 Hz images/plots_-3_+11_hz.png\n",
      "-3_+13 Hz images/plots_-3_+13_hz.png\n",
      "-6_+3 Hz images/plots_-6_+3_hz.png\n",
      "-6_+9 Hz images/plots_-6_+9_hz.png\n",
      "-6_+10 Hz images/plots_-6_+10_hz.png\n",
      "-6_+11 Hz images/plots_-6_+11_hz.png\n",
      "-6_+13 Hz images/plots_-6_+13_hz.png\n",
      "-9_+3 Hz images/plots_-9_+3_hz.png\n",
      "-9_+6 Hz images/plots_-9_+6_hz.png\n",
      "-9_+10 Hz images/plots_-9_+10_hz.png\n",
      "-9_+11 Hz images/plots_-9_+11_hz.png\n",
      "-9_+13 Hz images/plots_-9_+13_hz.png\n",
      "-10_+3 Hz images/plots_-10_+3_hz.png\n",
      "-10_+6 Hz images/plots_-10_+6_hz.png\n",
      "-10_+9 Hz images/plots_-10_+9_hz.png\n",
      "-10_+11 Hz images/plots_-10_+11_hz.png\n",
      "-10_+13 Hz images/plots_-10_+13_hz.png\n",
      "-11_+3 Hz images/plots_-11_+3_hz.png\n",
      "-11_+6 Hz images/plots_-11_+6_hz.png\n",
      "-11_+9 Hz images/plots_-11_+9_hz.png\n",
      "-11_+10 Hz images/plots_-11_+10_hz.png\n",
      "-11_+13 Hz images/plots_-11_+13_hz.png\n",
      "-13_+3 Hz images/plots_-13_+3_hz.png\n",
      "-13_+6 Hz images/plots_-13_+6_hz.png\n",
      "-13_+9 Hz images/plots_-13_+9_hz.png\n",
      "-13_+10 Hz images/plots_-13_+10_hz.png\n",
      "-13_+11 Hz images/plots_-13_+11_hz.png\n",
      "-10_+10 Hz images/plots_-10_+10_hz.png\n",
      "-9_+9 Hz images/plots_-9_+9_hz.png\n"
     ]
    }
   ],
   "source": [
    "for sched in schedule[3:-1]:\n",
    "title = f\"{sched[0]:+d}_{sched[1]:+d} Hz\"\n",
    "image_name = f\"images/plots_{sched[0]:+d}_{sched[1]:+d}_hz.png\"\n",
    "print(title, image_name)\n",
    "create_waterfall(title, \"  \", sched[2], 60, 1236524, 1236622, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
